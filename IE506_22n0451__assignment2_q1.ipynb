{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zu8KfKocVarc",
        "outputId": "ea5bf5da-e1ea-4972-95b4-4eac556eba11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (1.22.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (1.1.1)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (1.10.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (3.1.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oeozD8vxVark",
        "outputId": "f4a27d4a-5b3f-4e24-ad9b-ec1dbd07e47f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.9/dist-packages (0.12.2)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.17 in /usr/local/lib/python3.9/dist-packages (from seaborn) (1.22.4)\n",
            "Requirement already satisfied: pandas>=0.25 in /usr/local/lib/python3.9/dist-packages (from seaborn) (1.4.4)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in /usr/local/lib/python3.9/dist-packages (from seaborn) (3.7.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (23.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (8.4.0)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (5.12.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.11.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.0.9)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.0.7)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.39.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=0.25->seaborn) (2022.7.1)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib!=3.6.1,>=3.1->seaborn) (3.15.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6u63vi9LVarl",
        "outputId": "85877838-6231-4ab8-b329-f054906c82d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.9/dist-packages (0.8.10)\n"
          ]
        }
      ],
      "source": [
        "%pip install tabulate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "whLv3ZfX3IAb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "%matplotlib inline "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\huge \\text{Q1} \\\\ \\text{Write code to read the dataset in Data Q1.txt into two numpy arrays: X and y containing\n",
        "the features and labels respectively.}$"
      ],
      "metadata": {
        "id": "jVZQytQGYnGJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "R5cL5Yog3TAq"
      },
      "outputs": [],
      "source": [
        "with open('Data_Q1.txt') as f:\n",
        "    data_list = f.readlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfzdanvQ432Q",
        "outputId": "9c590e47-8496-42ea-b2c2-98731cd942d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[    1     2     3 ... 54875 54876 54877]\n",
            "{-1: 1933, 1: 2210}\n"
          ]
        }
      ],
      "source": [
        "labels=[]\n",
        "features=[]\n",
        "\n",
        "for line in data_list:\n",
        "  line_list=line.split()\n",
        "  labels.append(int(line_list[0]))\n",
        "  for j in line_list[1:]:\n",
        "    features.append(int(j.split(':')[0]))\n",
        "    \n",
        "features=np.array(features)\n",
        "labels_arr=np.unique(np.array(labels))\n",
        "features_arr=np.unique(features)\n",
        "print(features_arr)\n",
        "dict_count={}\n",
        "for i in labels_arr:\n",
        "  dict_count[i]=labels.count(i)\n",
        "print(dict_count)  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uAkQMZg6aNZ",
        "outputId": "f377e95e-a28f-4f6c-abc4-9472115ca4bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y Array: array of labels:\n",
            "\n",
            "[-1  1]\n",
            "\n",
            " X array: array of features:\n",
            "\n",
            "[    1     2     3 ... 54875 54876 54877]\n"
          ]
        }
      ],
      "source": [
        "print('y Array: array of labels:\\n')\n",
        "print(labels_arr)\n",
        "print('\\n X array: array of features:\\n')\n",
        "print(features_arr)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\huge \\text{Q2} \\\\ \\text{Print the number of classes in the data set and the number of samples belonging to\n",
        "each class. Indicate if there is class imbalance issue.}$"
      ],
      "metadata": {
        "id": "VQ5o9CCjYxUE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RC4N-kUQ9TAP",
        "outputId": "311b28fb-3f0a-4db5-976d-08cc40eddd6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of classes in dataset: 2 \n",
            "\n",
            "Number of features in dataset: 54877 \n",
            "\n",
            "Number of samples belonging to class  -1 is : 1933 \n",
            "\n",
            "Number of samples belonging to class  1 is : 2210 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "print('Number of classes in dataset:',len(labels_arr),'\\n')\n",
        "print('Number of features in dataset:',len(features_arr),'\\n')\n",
        "print('Number of samples belonging to class ',labels_arr[0],'is :',dict_count[labels_arr[0]],'\\n')\n",
        "print('Number of samples belonging to class ',labels_arr[1],'is :',dict_count[labels_arr[1]],'\\n')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-p4W3R1__gOL"
      },
      "outputs": [],
      "source": [
        "df=pd.DataFrame(labels,columns=['x'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "IM-fcd-UAI7W",
        "outputId": "de56658a-152a-4f28-e052-6763f606ea49"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkQAAAGwCAYAAABIC3rIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAeCElEQVR4nO3df5BV9X3/8dcC7oKBXYLCLhtXqrHBH0G0mOBOlfqD4YfG6MSm1VC1SnVqIR2zqTLMGDQ2DaMmamKp1raWJoOtSZqYBlMiooBV/BFaIhJl1MHBDC5QEVaIAsp+/+hwv1lRo7jsXfw8HjN3hnPOZ+99H2YWnnPP2bs1nZ2dnQEAKFifag8AAFBtgggAKJ4gAgCKJ4gAgOIJIgCgeIIIACieIAIAitev2gPsD3bt2pV169Zl0KBBqampqfY4AMB70NnZmVdffTXNzc3p0+fd3wMSRO/BunXr0tLSUu0xAIC98OKLL+aQQw551zWC6D0YNGhQkv/7C62vr6/yNADAe9HR0ZGWlpbK/+PvRhC9B7svk9XX1wsiANjPvJfbXdxUDQAUTxABAMUTRABA8QQRAFA8QQQAFE8QAQDFE0QAQPEEEQBQPEEEABRPEAEAxRNEAEDxBBEAUDxBBAAUTxABAMUTRABA8fpVewCAEqy9blS1R4Be6dBZK6s9QhLvEAEACCIAAEEEABRPEAEAxRNEAEDxBBEAUDxBBAAUTxABAMUTRABA8QQRAFA8QQQAFE8QAQDFE0QAQPEEEQBQPEEEABRPEAEAxRNEAEDxBBEAUDxBBAAUTxABAMUTRABA8QQRAFA8QQQAFE8QAQDFE0QAQPEEEQBQPEEEABRPEAEAxRNEAEDxqhpEs2fPzqc+9akMGjQow4YNyznnnJPVq1d3WfP6669n2rRpOeiggzJw4MCce+65Wb9+fZc1a9euzZlnnpkDDzwww4YNy5VXXpk33nijy5rFixfn937v91JXV5cjjjgic+fO3denBwDsJ6oaREuWLMm0adPy6KOPZuHChdm5c2cmTJiQbdu2VdZ86Utfyk9+8pN8//vfz5IlS7Ju3bp87nOfqxx/8803c+aZZ2bHjh155JFH8i//8i+ZO3duZs2aVVmzZs2anHnmmTn11FOzYsWKXHHFFfmzP/uz/OxnP+vR8wUAeqeazs7OzmoPsdvGjRszbNiwLFmyJOPGjcuWLVsydOjQ3HXXXfnDP/zDJMkzzzyTo446KsuWLcuJJ56Y//zP/8xnPvOZrFu3Lo2NjUmS22+/PTNmzMjGjRtTW1ubGTNm5N57781TTz1Vea3zzjsvmzdvzoIFC/aYY/v27dm+fXtlu6OjIy0tLdmyZUvq6+v38d8C8GG09rpR1R4BeqVDZ63cZ8/d0dGRhoaG9/T/d6+6h2jLli1JkiFDhiRJli9fnp07d2b8+PGVNUceeWQOPfTQLFu2LEmybNmyjBo1qhJDSTJx4sR0dHRk1apVlTW/+Ry71+x+jreaPXt2GhoaKo+WlpbuO0kAoNfpNUG0a9euXHHFFfn93//9fPKTn0yStLe3p7a2NoMHD+6ytrGxMe3t7ZU1vxlDu4/vPvZuazo6OvLaa6/tMcvMmTOzZcuWyuPFF1/slnMEAHqnftUeYLdp06blqaeeyn/9139Ve5TU1dWlrq6u2mMAAD2kV7xDNH369MyfPz8PPvhgDjnkkMr+pqam7NixI5s3b+6yfv369WlqaqqseetPne3e/m1r6uvrM2DAgO4+HQBgP1PVIOrs7Mz06dPzox/9KA888EAOO+ywLsfHjBmTAw44IIsWLarsW716ddauXZvW1tYkSWtra1auXJkNGzZU1ixcuDD19fU5+uijK2t+8zl2r9n9HABA2ap6yWzatGm566678uMf/ziDBg2q3PPT0NCQAQMGpKGhIVOnTk1bW1uGDBmS+vr6fPGLX0xra2tOPPHEJMmECRNy9NFH54ILLsgNN9yQ9vb2XH311Zk2bVrlstef//mf52//9m9z1VVX5ZJLLskDDzyQ733ve7n33nurdu4AQO9R1XeIbrvttmzZsiWnnHJKhg8fXnncfffdlTU333xzPvOZz+Tcc8/NuHHj0tTUlB/+8IeV43379s38+fPTt2/ftLa25k/+5E9y4YUX5rrrrqusOeyww3Lvvfdm4cKFGT16dL75zW/mH//xHzNx4sQePV8AoHfqVZ9D1Fu9n88xAHg7PocI3p7PIQIA6CUEEQBQPEEEABSv13wwI8mYK79T7RGgV1p+44XVHgH4kPMOEQBQPEEEABRPEAEAxRNEAEDxBBEAUDxBBAAUTxABAMUTRABA8QQRAFA8QQQAFE8QAQDFE0QAQPEEEQBQPEEEABRPEAEAxRNEAEDxBBEAUDxBBAAUTxABAMUTRABA8QQRAFA8QQQAFE8QAQDFE0QAQPEEEQBQPEEEABRPEAEAxRNEAEDxBBEAUDxBBAAUTxABAMUTRABA8QQRAFA8QQQAFE8QAQDFE0QAQPEEEQBQPEEEABRPEAEAxRNEAEDxBBEAUDxBBAAUTxABAMUTRABA8QQRAFA8QQQAFE8QAQDFE0QAQPEEEQBQPEEEABRPEAEAxRNEAEDxBBEAUDxBBAAUTxABAMUTRABA8QQRAFA8QQQAFE8QAQDFE0QAQPEEEQBQPEEEABRPEAEAxRNEAEDxqhpES5cuzVlnnZXm5ubU1NTknnvu6XL8T//0T1NTU9PlMWnSpC5rNm3alClTpqS+vj6DBw/O1KlTs3Xr1i5rnnzyyZx88snp379/WlpacsMNN+zrUwMA9iNVDaJt27Zl9OjRmTNnzjuumTRpUl566aXK41//9V+7HJ8yZUpWrVqVhQsXZv78+Vm6dGkuu+yyyvGOjo5MmDAhI0aMyPLly3PjjTfm2muvzR133LHPzgsA2L/0q+aLT548OZMnT37XNXV1dWlqanrbY08//XQWLFiQJ554IieccEKS5NZbb80ZZ5yRb3zjG2lubs68efOyY8eO3Hnnnamtrc0xxxyTFStW5KabbuoSTgBAuXr9PUSLFy/OsGHDMnLkyFx++eV5+eWXK8eWLVuWwYMHV2IoScaPH58+ffrkscceq6wZN25camtrK2smTpyY1atX55VXXnnb19y+fXs6Ojq6PACAD69eHUSTJk3Kd77znSxatCjXX399lixZksmTJ+fNN99MkrS3t2fYsGFdvqZfv34ZMmRI2tvbK2saGxu7rNm9vXvNW82ePTsNDQ2VR0tLS3efGgDQi1T1ktlvc95551X+PGrUqBx77LH5+Mc/nsWLF+f000/fZ687c+bMtLW1VbY7OjpEEQB8iPXqd4je6vDDD8/BBx+c5557LknS1NSUDRs2dFnzxhtvZNOmTZX7jpqamrJ+/foua3Zvv9O9SXV1damvr+/yAAA+vParIPrVr36Vl19+OcOHD0+StLa2ZvPmzVm+fHllzQMPPJBdu3Zl7NixlTVLly7Nzp07K2sWLlyYkSNH5qMf/WjPngAA0CtVNYi2bt2aFStWZMWKFUmSNWvWZMWKFVm7dm22bt2aK6+8Mo8++mheeOGFLFq0KGeffXaOOOKITJw4MUly1FFHZdKkSbn00kvz+OOP5+GHH8706dNz3nnnpbm5OUnyhS98IbW1tZk6dWpWrVqVu+++O9/61re6XBIDAMpW1SD6+c9/nuOPPz7HH398kqStrS3HH398Zs2alb59++bJJ5/MZz/72XziE5/I1KlTM2bMmDz00EOpq6urPMe8efNy5JFH5vTTT88ZZ5yRk046qctnDDU0NOS+++7LmjVrMmbMmHz5y1/OrFmz/Mg9AFBR1ZuqTznllHR2dr7j8Z/97Ge/9TmGDBmSu+66613XHHvssXnooYfe93wAQBn2q3uIAAD2BUEEABRPEAEAxRNEAEDxBBEAUDxBBAAUTxABAMUTRABA8QQRAFA8QQQAFE8QAQDFE0QAQPEEEQBQPEEEABRPEAEAxRNEAEDxBBEAUDxBBAAUTxABAMUTRABA8QQRAFA8QQQAFE8QAQDFE0QAQPEEEQBQPEEEABRPEAEAxRNEAEDxBBEAUDxBBAAUb6+C6LTTTsvmzZv32N/R0ZHTTjvtg84EANCj9iqIFi9enB07duyx//XXX89DDz30gYcCAOhJ/d7P4ieffLLy51/+8pdpb2+vbL/55ptZsGBBPvaxj3XfdAAAPeB9BdFxxx2Xmpqa1NTUvO2lsQEDBuTWW2/ttuEAAHrC+wqiNWvWpLOzM4cffngef/zxDB06tHKstrY2w4YNS9++fbt9SACAfel9BdGIESOSJLt27donwwAAVMP7CqLf9Oyzz+bBBx/Mhg0b9gikWbNmfeDBAAB6yl4F0T/8wz/k8ssvz8EHH5ympqbU1NRUjtXU1AgiAGC/sldB9LWvfS1/8zd/kxkzZnT3PAAAPW6vPofolVdeyec///nungUAoCr2Kog+//nP57777uvuWQAAqmKvLpkdccQR+cpXvpJHH300o0aNygEHHNDl+F/+5V92y3AAAD1hr4LojjvuyMCBA7NkyZIsWbKky7GamhpBBADsV/YqiNasWdPdcwAAVM1e3UMEAPBhslfvEF1yySXvevzOO+/cq2EAAKphr4LolVde6bK9c+fOPPXUU9m8efPb/tJXAIDebK+C6Ec/+tEe+3bt2pXLL788H//4xz/wUAAAPanb7iHq06dP2tracvPNN3fXUwIA9Ihuvan6+eefzxtvvNGdTwkAsM/t1SWztra2LtudnZ156aWXcu+99+aiiy7qlsEAAHrKXgXR//zP/3TZ7tOnT4YOHZpvfvObv/Un0AAAepu9CqIHH3ywu+cAAKiavQqi3TZu3JjVq1cnSUaOHJmhQ4d2y1AAAD1pr26q3rZtWy655JIMHz4848aNy7hx49Lc3JypU6fm17/+dXfPCACwT+1VELW1tWXJkiX5yU9+ks2bN2fz5s358Y9/nCVLluTLX/5yd88IALBP7dUls3//93/PD37wg5xyyimVfWeccUYGDBiQP/qjP8ptt93WXfMBAOxze/UO0a9//es0NjbusX/YsGEumQEA+529CqLW1tZcc801ef311yv7XnvttXz1q19Na2trtw0HANAT9uqS2S233JJJkyblkEMOyejRo5Mkv/jFL1JXV5f77ruvWwcEANjX9iqIRo0alWeffTbz5s3LM888kyQ5//zzM2XKlAwYMKBbBwQA2Nf2Kohmz56dxsbGXHrppV3233nnndm4cWNmzJjRLcMBAPSEvbqH6O///u9z5JFH7rH/mGOOye233/6BhwIA6El7FUTt7e0ZPnz4HvuHDh2al1566QMPBQDQk/YqiFpaWvLwww/vsf/hhx9Oc3PzBx4KAKAn7dU9RJdeemmuuOKK7Ny5M6eddlqSZNGiRbnqqqt8UjUAsN/ZqyC68sor8/LLL+cv/uIvsmPHjiRJ//79M2PGjMycObNbBwQA2Nf2Kohqampy/fXX5ytf+UqefvrpDBgwIL/7u7+burq67p4PAGCf26sg2m3gwIH51Kc+1V2zAABUxV7dVN1dli5dmrPOOivNzc2pqanJPffc0+V4Z2dnZs2aleHDh2fAgAEZP358nn322S5rNm3alClTpqS+vj6DBw/O1KlTs3Xr1i5rnnzyyZx88snp379/WlpacsMNN+zrUwMA9iNVDaJt27Zl9OjRmTNnztsev+GGG/Ltb387t99+ex577LF85CMfycSJE7v8DrUpU6Zk1apVWbhwYebPn5+lS5fmsssuqxzv6OjIhAkTMmLEiCxfvjw33nhjrr322txxxx37/PwAgP3DB7pk9kFNnjw5kydPfttjnZ2dueWWW3L11Vfn7LPPTpJ85zvfSWNjY+65556cd955efrpp7NgwYI88cQTOeGEE5Ikt956a84444x84xvfSHNzc+bNm5cdO3bkzjvvTG1tbY455pisWLEiN910U5dw+k3bt2/P9u3bK9sdHR3dfOYAQG9S1XeI3s2aNWvS3t6e8ePHV/Y1NDRk7NixWbZsWZJk2bJlGTx4cCWGkmT8+PHp06dPHnvsscqacePGpba2trJm4sSJWb16dV555ZW3fe3Zs2enoaGh8mhpadkXpwgA9BK9Noja29uTJI2NjV32NzY2Vo61t7dn2LBhXY7369cvQ4YM6bLm7Z7jN1/jrWbOnJktW7ZUHi+++OIHPyEAoNeq6iWz3qqurs5HCABAQXrtO0RNTU1JkvXr13fZv379+sqxpqambNiwocvxN954I5s2beqy5u2e4zdfAwAoW68NosMOOyxNTU1ZtGhRZV9HR0cee+yxtLa2JklaW1uzefPmLF++vLLmgQceyK5duzJ27NjKmqVLl2bnzp2VNQsXLszIkSPz0Y9+tIfOBgDozaoaRFu3bs2KFSuyYsWKJP93I/WKFSuydu3a1NTU5IorrsjXvva1/Md//EdWrlyZCy+8MM3NzTnnnHOSJEcddVQmTZqUSy+9NI8//ngefvjhTJ8+Peedd17ll8x+4QtfSG1tbaZOnZpVq1bl7rvvzre+9a20tbVV6awBgN6mqvcQ/fznP8+pp55a2d4dKRdddFHmzp2bq666Ktu2bctll12WzZs356STTsqCBQvSv3//ytfMmzcv06dPz+mnn54+ffrk3HPPzbe//e3K8YaGhtx3332ZNm1axowZk4MPPjizZs16xx+5BwDKU9PZ2dlZ7SF6u46OjjQ0NGTLli2pr6/fZ68z5srv7LPnhv3Z8hsvrPYIH9ja60ZVewTolQ6dtXKfPff7+f+7195DBADQUwQRAFA8QQQAFE8QAQDFE0QAQPEEEQBQPEEEABRPEAEAxRNEAEDxBBEAUDxBBAAUTxABAMUTRABA8QQRAFA8QQQAFE8QAQDFE0QAQPEEEQBQPEEEABRPEAEAxRNEAEDxBBEAUDxBBAAUTxABAMUTRABA8QQRAFA8QQQAFE8QAQDFE0QAQPEEEQBQPEEEABRPEAEAxRNEAEDxBBEAUDxBBAAUTxABAMUTRABA8QQRAFA8QQQAFE8QAQDFE0QAQPEEEQBQPEEEABRPEAEAxRNEAEDxBBEAUDxBBAAUTxABAMUTRABA8QQRAFA8QQQAFE8QAQDFE0QAQPEEEQBQPEEEABRPEAEAxRNEAEDxBBEAUDxBBAAUTxABAMUTRABA8QQRAFA8QQQAFE8QAQDFE0QAQPEEEQBQPEEEABRPEAEAxevVQXTttdempqamy+PII4+sHH/99dczbdq0HHTQQRk4cGDOPffcrF+/vstzrF27NmeeeWYOPPDADBs2LFdeeWXeeOONnj4VAKAX61ftAX6bY445Jvfff39lu1+//z/yl770pdx77735/ve/n4aGhkyfPj2f+9zn8vDDDydJ3nzzzZx55plpamrKI488kpdeeikXXnhhDjjggHz961/v8XMBAHqnXh9E/fr1S1NT0x77t2zZkn/6p3/KXXfdldNOOy1J8s///M856qij8uijj+bEE0/Mfffdl1/+8pe5//7709jYmOOOOy5//dd/nRkzZuTaa69NbW1tT58OANAL9epLZkny7LPPprm5OYcffnimTJmStWvXJkmWL1+enTt3Zvz48ZW1Rx55ZA499NAsW7YsSbJs2bKMGjUqjY2NlTUTJ05MR0dHVq1a9Y6vuX379nR0dHR5AAAfXr06iMaOHZu5c+dmwYIFue2227JmzZqcfPLJefXVV9Pe3p7a2toMHjy4y9c0Njamvb09SdLe3t4lhnYf333sncyePTsNDQ2VR0tLS/eeGADQq/TqS2aTJ0+u/PnYY4/N2LFjM2LEiHzve9/LgAED9tnrzpw5M21tbZXtjo4OUQQAH2K9+h2itxo8eHA+8YlP5LnnnktTU1N27NiRzZs3d1mzfv36yj1HTU1Ne/zU2e7tt7svabe6urrU19d3eQAAH177VRBt3bo1zz//fIYPH54xY8bkgAMOyKJFiyrHV69enbVr16a1tTVJ0trampUrV2bDhg2VNQsXLkx9fX2OPvroHp8fAOidevUls7/6q7/KWWedlREjRmTdunW55ppr0rdv35x//vlpaGjI1KlT09bWliFDhqS+vj5f/OIX09ramhNPPDFJMmHChBx99NG54IILcsMNN6S9vT1XX311pk2blrq6uiqfHQDQW/TqIPrVr36V888/Py+//HKGDh2ak046KY8++miGDh2aJLn55pvTp0+fnHvuudm+fXsmTpyYv/u7v6t8fd++fTN//vxcfvnlaW1tzUc+8pFcdNFFue6666p1SgBAL9Srg+jf/u3f3vV4//79M2fOnMyZM+cd14wYMSI//elPu3s0AOBDZL+6hwgAYF8QRABA8QQRAFA8QQQAFE8QAQDFE0QAQPEEEQBQPEEEABRPEAEAxRNEAEDxBBEAUDxBBAAUTxABAMUTRABA8QQRAFA8QQQAFE8QAQDFE0QAQPEEEQBQPEEEABRPEAEAxRNEAEDxBBEAUDxBBAAUTxABAMUTRABA8QQRAFA8QQQAFE8QAQDFE0QAQPEEEQBQPEEEABRPEAEAxRNEAEDxBBEAUDxBBAAUTxABAMUTRABA8QQRAFA8QQQAFE8QAQDFE0QAQPEEEQBQPEEEABRPEAEAxRNEAEDxBBEAUDxBBAAUTxABAMUTRABA8QQRAFA8QQQAFE8QAQDFE0QAQPEEEQBQPEEEABRPEAEAxRNEAEDxBBEAUDxBBAAUTxABAMUTRABA8QQRAFA8QQQAFE8QAQDFE0QAQPEEEQBQPEEEABRPEAEAxSsqiObMmZPf+Z3fSf/+/TN27Ng8/vjj1R4JAOgFigmiu+++O21tbbnmmmvy3//93xk9enQmTpyYDRs2VHs0AKDKigmim266KZdeemkuvvjiHH300bn99ttz4IEH5s4776z2aABAlfWr9gA9YceOHVm+fHlmzpxZ2denT5+MHz8+y5Yt22P99u3bs3379sr2li1bkiQdHR37dM43t7+2T58f9lf7+nuvJ7z6+pvVHgF6pX35/b37uTs7O3/r2iKC6H//93/z5ptvprGxscv+xsbGPPPMM3usnz17dr761a/usb+lpWWfzQi8s4Zb/7zaIwD7yuyGff4Sr776ahoa3v11igii92vmzJlpa2urbO/atSubNm3KQQcdlJqamipORk/o6OhIS0tLXnzxxdTX11d7HKAb+f4uS2dnZ1599dU0Nzf/1rVFBNHBBx+cvn37Zv369V32r1+/Pk1NTXusr6urS11dXZd9gwcP3pcj0gvV19f7BxM+pHx/l+O3vTO0WxE3VdfW1mbMmDFZtGhRZd+uXbuyaNGitLa2VnEyAKA3KOIdoiRpa2vLRRddlBNOOCGf/vSnc8stt2Tbtm25+OKLqz0aAFBlxQTRH//xH2fjxo2ZNWtW2tvbc9xxx2XBggV73GgNdXV1ueaaa/a4bArs/3x/805qOt/Lz6IBAHyIFXEPEQDAuxFEAEDxBBEAUDxBBAAUTxDBW/zwhz/MhAkTKp9MvmLFimqPBHSDpUuX5qyzzkpzc3Nqampyzz33VHskehFBBG+xbdu2nHTSSbn++uurPQrQjbZt25bRo0dnzpw51R6FXqiYzyGC9+qCCy5IkrzwwgvVHQToVpMnT87kyZOrPQa9lHeIAIDiCSIAoHiCiKLNmzcvAwcOrDweeuihao8EQBW4h4iiffazn83YsWMr2x/72MeqOA0A1SKIKNqgQYMyaNCgao8BQJUJIniLTZs2Ze3atVm3bl2SZPXq1UmSpqamNDU1VXM04APYunVrnnvuucr2mjVrsmLFigwZMiSHHnpoFSejN/Db7uEt5s6dm4svvniP/ddcc02uvfbanh8I6BaLFy/Oqaeeusf+iy66KHPnzu35gehVBBEAUDw/ZQYAFE8QAQDFE0QAQPEEEQBQPEEEABRPEAEAxRNEAEDxBBEAUDxBBAAUTxABAMUTRABA8QQRUKSNGzemqakpX//61yv7HnnkkdTW1mbRokVVnAyoBr/cFSjWT3/605xzzjl55JFHMnLkyBx33HE5++yzc9NNN1V7NKCHCSKgaNOmTcv999+fE044IStXrswTTzyRurq6ao8F9DBBBBTttddeyyc/+cm8+OKLWb58eUaNGlXtkYAqcA8RULTnn38+69aty65du/LCCy9UexygSrxDBBRrx44d+fSnP53jjjsuI0eOzC233JKVK1dm2LBh1R4N6GGCCCjWlVdemR/84Af5xS9+kYEDB+YP/uAP0tDQkPnz51d7NKCHuWQGFGnx4sW55ZZb8t3vfjf19fXp06dPvvvd7+ahhx7KbbfdVu3xgB7mHSIAoHjeIQIAiieIAIDiCSIAoHiCCAAoniACAIoniACA4gkiAKB4gggAKJ4gAgCKJ4gAgOIJIgCgeP8PUhFJ4m47hf4AAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "ax=sns.countplot(x='x',data=df)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Proportion of class 1 in data:',dict_count[labels_arr[1]]/(dict_count[labels_arr[1]]+dict_count[labels_arr[0]]),'%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JD52OXoQWO8-",
        "outputId": "332bc16f-6e35-40b2-ad93-4197e5df2d67"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Proportion of class 1 in data: 0.5334298817282163 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Proportion of class -1 in data:',dict_count[labels_arr[0]]/(dict_count[labels_arr[1]]+dict_count[labels_arr[0]]),'%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArTVlTuwWmUM",
        "outputId": "e4804748-5146-4589-a113-c90e93093f55"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Proportion of class -1 in data: 0.46657011827178374 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\text{As we can see that proportion of class 1 and -1 in data is lying in interval } [40,60] \\text{So our data is in balanced mode.}$"
      ],
      "metadata": {
        "id": "-F2M_UtvWtOv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCxqlQGOAZcP"
      },
      "source": [
        "$\\large \\text{Here we can see that Classes balance.}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 78
        },
        "id": "0FWeb4qmAW20",
        "outputId": "e4c16da2-9f49-414d-db80-29042b3d571b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [label, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, ...]\n",
              "Index: []\n",
              "\n",
              "[0 rows x 54878 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-02fa9a47-83d6-40f2-b7ce-764be1b02055\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>54868</th>\n",
              "      <th>54869</th>\n",
              "      <th>54870</th>\n",
              "      <th>54871</th>\n",
              "      <th>54872</th>\n",
              "      <th>54873</th>\n",
              "      <th>54874</th>\n",
              "      <th>54875</th>\n",
              "      <th>54876</th>\n",
              "      <th>54877</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>0 rows × 54878 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-02fa9a47-83d6-40f2-b7ce-764be1b02055')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-02fa9a47-83d6-40f2-b7ce-764be1b02055 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-02fa9a47-83d6-40f2-b7ce-764be1b02055');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "df=pd.DataFrame(columns=['label']+list(features_arr))\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLfLEKNjG55K",
        "outputId": "1f29018f-a26a-419f-f938-7c7dbd676184"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of data points: 4143\n"
          ]
        }
      ],
      "source": [
        "# number of data points\n",
        "n=len(data_list)\n",
        "print('number of data points:',n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_5nJpeBHCpR",
        "outputId": "c7539f9e-8954-442f-aff5-1a368f415869"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of features: 54877\n"
          ]
        }
      ],
      "source": [
        "#number of features\n",
        "d=len(features_arr)\n",
        "print('number of features:',d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTaQboKqIEDO",
        "outputId": "14de78a1-e1c0-4e52-b17c-3c61f443ea5a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "array=np.array([0 for i in range(n*(d+1))]).reshape(n,(d+1))\n",
        "array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8QagxqkBCfv",
        "outputId": "a46da696-73a2-451a-beaa-67f7ae001c7a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1,  1,  1, ...,  0,  0,  0],\n",
              "       [-1,  0,  0, ...,  0,  0,  0],\n",
              "       [-1,  0,  0, ...,  0,  0,  0],\n",
              "       ...,\n",
              "       [ 1,  0,  0, ...,  0,  0,  0],\n",
              "       [-1,  0,  0, ...,  1,  1,  1],\n",
              "       [ 1,  0,  0, ...,  0,  0,  0]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "\n",
        "for i in range(len(data_list)):\n",
        "  line=data_list[i].split()\n",
        "  array[i,0]=int(line[0])\n",
        "  for j in line[1:]:\n",
        "    a=j.split(':')\n",
        "    array[i,int(a[0])]=int(a[1])\n",
        "array   \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "YJSYrL5aIXi3"
      },
      "outputs": [],
      "source": [
        "df=pd.DataFrame(array,columns=['label']+list(features_arr))\n",
        "data=df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Uh7KXUb3Il68"
      },
      "outputs": [],
      "source": [
        "#data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "qosTeMx7-Ajy"
      },
      "outputs": [],
      "source": [
        "del array\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\huge \\text{Q3.} \\\\ \\text{Split the data into two sets such that 80% of the data is considered as set T1 and\n",
        "20% of the data is considered as set T2. Justify if set T1 and} \\\\ \\text{ set T2 have similar class label proportions.}$"
      ],
      "metadata": {
        "id": "1xcfMQ79Y-1W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "O7Hkmo3XJu1A"
      },
      "outputs": [],
      "source": [
        "\n",
        "data_X=data[data.columns[1:]]\n",
        "data_Y=data['label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "bXvyy2FCItWb"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,Y_train,Y_test=train_test_split(data_X,data_Y,test_size=0.2,random_state=34)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKNeEo6kKOYE",
        "outputId": "ced09fea-f4a0-4715-cfd8-d14232095b42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training data points: 3314 ,test data points: 829\n"
          ]
        }
      ],
      "source": [
        "print('training data points:',len(X_train),',test data points:',len(X_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBVdGI37KdPq",
        "outputId": "cce6bbf2-8e9e-4d5f-b499-95e5f7951a2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "proportion of label 1 in training data set: 0.5331925165962583\n",
            "proportion of label -1 in training data set: 0.4668074834037417\n",
            "proportion of label 1 in training data set: 0.53437876960193\n",
            "proportion of label -1 in training data set: 0.46562123039806996\n"
          ]
        }
      ],
      "source": [
        "# Label proportion\n",
        "label=list(Y_train)\n",
        "proportion_1=label.count(1)/len(Y_train)\n",
        "proportion_2=label.count(-1)/len(Y_train)\n",
        "del label\n",
        "print('proportion of label 1 in training data set:',proportion_1)\n",
        "print('proportion of label -1 in training data set:',proportion_2)\n",
        "label=list(Y_test)\n",
        "proportion_1=label.count(1)/len(Y_test)\n",
        "proportion_2=label.count(-1)/len(Y_test)\n",
        "del label\n",
        "print('proportion of label 1 in training data set:',proportion_1)\n",
        "print('proportion of label -1 in training data set:',proportion_2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2ILhS8xi_Jr"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import cross_val_score, GridSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score,precision_score,recall_score,confusion_matrix\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZ3nEIvwSkuS"
      },
      "source": [
        "$\\large \\text{Parameter values:}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zyYFyTBVVars"
      },
      "outputs": [],
      "source": [
        "from timeit import default_timer as timer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPSq7pefjC-E"
      },
      "outputs": [],
      "source": [
        "# Define a range of possible values for the hyperparameter C\n",
        "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_2heOrNVars"
      },
      "source": [
        "$\\huge \\text{Qd.} \\\\  \\text{Using T1 as training data, train each of the following algorithms by tuning only the\n",
        "hyperparameters specified below (keep all other} \\\\ \\text{ hyperparameters fixed to the default values in\n",
        "scikit-learn)} \\\\ \\text{:\n",
        "i. Logistic regression with L2 regularizer (Hyperparameter: regularization constant C)} \\\\ \\text{\n",
        "ii. Logistic regression with L1 regularizer (Hyperparameter: regularization constant C)} \\\\ \\text{\n",
        "iii. Soft-margin SVM with L2 regularizer (Hyperparameter: regularization constant C)} \\\\ \\text{\n",
        "iv. Soft-margin SVM with L1 regularizer (Hyperparameter: regularization constant C)} \\\\ \\text{\n",
        "v. Kernel SVM with RBF kernel (Hyperparameter: kernel parameter γ)} \\\\ \\text{\n",
        "vi. KNN (Hyperparameter: number of neighbors)} \\\\ \\text{\n",
        "vii. Decision tree (Hyperparameter: min weight fraction leaf)} \\\\ \\text{\n",
        "viii. Random forest (Hyperparameter: number of estimators)} \\\\ \\text{\n",
        "Choose appropriate ranges for the hyperparameters to be tuned. Clearly indicate the range you\n",
        "choose and justify your choice for the range} \\\\ \\text{ chosen. Tune the hyperparameters using 5-fold\n",
        "cross-validation procedure.}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMldE5NPSs_7"
      },
      "source": [
        "$\\huge \\text{Logistic Regression with penalty L2}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gy7qWOQXVars"
      },
      "outputs": [],
      "source": [
        "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nTFlr-KVars"
      },
      "source": [
        "$\\large \\text{Reason for choosing these parameters:} \\\\ \\text{ Since we want to see the penalty effect over the prediction so we want to take a broad range of penalty so we started from } 10^{-3} \\text{and goes to} 10^4. \\\\  \\text{So that we can find the best parameter. }$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_xyRlvIjGOd",
        "outputId": "8c872c57-59de-4c7c-a81c-3de9a308a13e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best C: 1\n",
            "For Logistic regression using l2 penalty:\n",
            "\n",
            "for training data \n",
            " ******************************************\n",
            "Accuracy is: 0.9981894990947495\n",
            "Precision is: 0.99830220713073\n",
            "Recall is: 0.99830220713073\n",
            "specificity is : 0.9980607627666451\n",
            "sensitivity is: 0.99830220713073\n",
            "\n",
            " ****************************************\n",
            " for test data\n",
            "Accuracy is: 0.8914354644149578\n",
            "Precision is: 0.8812095032397408\n",
            "Recall is: 0.9209932279909706\n",
            "specificity is : 0.8575129533678757\n",
            "sensitivity is: 0.9209932279909706\n",
            "***********************************************************\n",
            "*********************************************\n",
            "Time taken : 307.73681197687984\n"
          ]
        }
      ],
      "source": [
        "# Use cross-validation on the training set to evaluate the performance of the logistic regression model for each value of C\n",
        "start= timer()\n",
        "lr = LogisticRegression(penalty='l2',max_iter=1000,solver='lbfgs')\n",
        "grid_search = GridSearchCV(lr, param_grid,  cv=5,n_jobs=multiprocessing.cpu_count())\n",
        "grid_search.fit(X_train, Y_train)\n",
        "\n",
        "# Select the optimal value of C based on the performance of the model on the validation set\n",
        "best_C = grid_search.best_params_['C']\n",
        "print(f\"Best C: {best_C}\")\n",
        "\n",
        "# Train the final logistic regression model using the selected value of C and the entire training set\n",
        "final_lr = LogisticRegression(penalty='l2',  C=best_C)\n",
        "final_lr.fit(X_train, Y_train)\n",
        "\n",
        "# for training data \n",
        "\n",
        "y_pred_train=final_lr.predict(X_train)\n",
        "accuracy_lr_l2_train = accuracy_score(Y_train, y_pred_train)\n",
        "precision_lr_l2_train=precision_score(Y_train,y_pred_train)\n",
        "recall_lr_l2_train=recall_score(Y_train,y_pred_train)\n",
        "# finding confusion matrix\n",
        "tn, fp, fn, tp = confusion_matrix(Y_train, y_pred_train).ravel()\n",
        "# specificity\n",
        "specificity_lr_l2_train=tn/(tn+fp)\n",
        "# sensitivity\n",
        "sensitivity_lr_l2_train=tp/(tp+fn)\n",
        "# print the all results\n",
        "\n",
        "print('For Logistic regression using l2 penalty:\\n')\n",
        "print('for training data \\n ******************************************')\n",
        "print('Accuracy is:',accuracy_lr_l2_train)\n",
        "print('Precision is:',precision_lr_l2_train)\n",
        "print('Recall is:',recall_lr_l2_train)\n",
        "print('specificity is :',specificity_lr_l2_train)\n",
        "print('sensitivity is:',sensitivity_lr_l2_train)\n",
        "\n",
        "# for test  data \n",
        "y_pred_test = final_lr.predict(X_test)\n",
        "accuracy_lr_l2_test = accuracy_score(Y_test, y_pred_test)\n",
        "precision_lr_l2_test=precision_score(Y_test,y_pred_test)\n",
        "recall_lr_l2_test=recall_score(Y_test,y_pred_test)\n",
        "# finding confusion matrix\n",
        "tn, fp, fn, tp = confusion_matrix(Y_test, y_pred_test).ravel()\n",
        "# specificity\n",
        "specificity_lr_l2_test=tn/(tn+fp)\n",
        "# sensitivity\n",
        "sensitivity_lr_l2_test=tp/(tp+fn)\n",
        "end=timer()\n",
        "# print the all results\n",
        "print('\\n ****************************************\\n for test data')\n",
        "print('Accuracy is:',accuracy_lr_l2_test)\n",
        "print('Precision is:',precision_lr_l2_test)\n",
        "print('Recall is:',recall_lr_l2_test)\n",
        "print('specificity is :',specificity_lr_l2_test)\n",
        "print('sensitivity is:',sensitivity_lr_l2_test)\n",
        "print('***********************************************************\\n*********************************************')\n",
        "print('Time taken :',end-start)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFmjI5ppS1vr"
      },
      "source": [
        "$\\huge \\text{Logistic Regression with L1 Norm}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEXMpp34Vart"
      },
      "outputs": [],
      "source": [
        "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJybBQUbVart"
      },
      "source": [
        "$\\large \\text{Reason for choosing these parameters:} \\\\ \\text{ Since we want to see the penalty effect over the prediction so we want to take a broad range of penalty so we started from } 10^{-3} \\text{and goes to} 10^4. \\\\  \\text{So that we can find the best parameter. }$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVhS35rVFSAS",
        "outputId": "6618d07e-6772-4363-ddab-c2d2c82c84ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best C: 1000\n",
            "For Logistic regression using l1 penalty:\n",
            "\n",
            "for training data : \n",
            " ************************************\n",
            "Accuracy is: 0.9993964996982498\n",
            "Precision is: 0.9994340690435767\n",
            "Recall is: 0.9994340690435767\n",
            "specificity is : 0.9993535875888817\n",
            "sensitivity is: 0.9994340690435767\n",
            "************************************************\n",
            " for test data:\n",
            "Accuracy is: 0.8841978287092883\n",
            "Precision is: 0.8864142538975501\n",
            "Recall is: 0.8984198645598194\n",
            "specificity is : 0.8678756476683938\n",
            "sensitivity is: 0.8984198645598194\n",
            "***********************************************************\n",
            "*********************************************\n",
            "time taken is: 117.35022687539458\n"
          ]
        }
      ],
      "source": [
        "# Use cross-validation on the training set to evaluate the performance of the logistic regression model for each value of C\n",
        "start=timer()\n",
        "lr = LogisticRegression(penalty='l1', solver='liblinear')\n",
        "grid_search = GridSearchCV(lr, param_grid,  cv=5)\n",
        "grid_search.fit(X_train, Y_train)\n",
        "\n",
        "# Select the optimal value of C based on the performance of the model on the validation set\n",
        "best_C = grid_search.best_params_['C']\n",
        "print(f\"Best C: {best_C}\")\n",
        "\n",
        "# Train the final logistic regression model using the selected value of C and the entire training set\n",
        "final_lr = LogisticRegression(penalty='l1', C=best_C,solver='liblinear')\n",
        "final_lr.fit(X_train, Y_train)\n",
        "\n",
        "# Evaluate the performance of the final model on the test set to estimate its performance on unseen data\n",
        "y_pred_test = final_lr.predict(X_test)\n",
        "\n",
        "# for training data \n",
        "\n",
        "y_pred_train=final_lr.predict(X_train)\n",
        "accuracy_lr_l1_train = accuracy_score(Y_train, y_pred_train)\n",
        "precision_lr_l1_train=precision_score(Y_train,y_pred_train)\n",
        "recall_lr_l1_train=recall_score(Y_train,y_pred_train)\n",
        "# finding confusion matrix\n",
        "tn, fp, fn, tp = confusion_matrix(Y_train, y_pred_train).ravel()\n",
        "# specificity\n",
        "specificity_lr_l1_train=tn/(tn+fp)\n",
        "# sensitivity\n",
        "sensitivity_lr_l1_train=tp/(tp+fn)\n",
        "# print the all results\n",
        "print('For Logistic regression using l1 penalty:\\n')\n",
        "print('for training data : \\n ************************************')\n",
        "print('Accuracy is:',accuracy_lr_l1_train)\n",
        "print('Precision is:',precision_lr_l1_train)\n",
        "print('Recall is:',recall_lr_l1_train)\n",
        "print('specificity is :',specificity_lr_l1_train)\n",
        "print('sensitivity is:',sensitivity_lr_l1_train)\n",
        "\n",
        "# for test  data \n",
        "y_pred_test = final_lr.predict(X_test)\n",
        "accuracy_lr_l1_test = accuracy_score(Y_test, y_pred_test)\n",
        "precision_lr_l1_test=precision_score(Y_test,y_pred_test)\n",
        "recall_lr_l1_test=recall_score(Y_test,y_pred_test)\n",
        "# finding confusion matrix\n",
        "tn, fp, fn, tp = confusion_matrix(Y_test, y_pred_test).ravel()\n",
        "# specificity\n",
        "specificity_lr_l1_test=tn/(tn+fp)\n",
        "# sensitivity\n",
        "sensitivity_lr_l1_test=tp/(tp+fn)\n",
        "end=timer()\n",
        "# print the all results\n",
        "print('************************************************\\n for test data:')\n",
        "print('Accuracy is:',accuracy_lr_l1_test)\n",
        "print('Precision is:',precision_lr_l1_test)\n",
        "print('Recall is:',recall_lr_l1_test)\n",
        "print('specificity is :',specificity_lr_l1_test)\n",
        "print('sensitivity is:',sensitivity_lr_l1_test)\n",
        "print('***********************************************************\\n*********************************************')\n",
        "print('time taken is:',end-start)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97fN3B_hYSek"
      },
      "source": [
        "$\\huge \\text{Soft margin SVM with L2 norm:}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyCNjySFVart"
      },
      "outputs": [],
      "source": [
        "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLJTA3ivVaru"
      },
      "source": [
        "$\\large \\text{Reason for choosing these parameters:} \\\\ \\text{ Since we want to see the penalty effect over the prediction so we want to take a broad range of penalty so we started from } 10^{-3} \\text{and goes to} 10^4. \\\\  \\text{So that we can find the best parameter. }$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ttGBneQFZdP",
        "outputId": "ee52707e-8af8-4e75-d645-8befaa25ff80"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/22n0451/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/home/22n0451/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/home/22n0451/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/home/22n0451/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/home/22n0451/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/home/22n0451/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/home/22n0451/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/home/22n0451/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/home/22n0451/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/home/22n0451/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/home/22n0451/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/home/22n0451/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/home/22n0451/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/home/22n0451/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/home/22n0451/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/home/22n0451/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/home/22n0451/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/home/22n0451/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/home/22n0451/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/home/22n0451/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best C: 0.01\n",
            "For soft svm  using l2 penalty:\n",
            "\n",
            "for training data : \n",
            " ************************************\n",
            "Accuracy is: 0.9903439951719976\n",
            "Precision is: 0.9887323943661972\n",
            "Recall is: 0.9932088285229203\n",
            "specificity is : 0.9870717517776342\n",
            "sensitivity is: 0.9932088285229203\n",
            "************************************************\n",
            " for test data:\n",
            "Accuracy is: 0.8902291917973462\n",
            "Precision is: 0.8713080168776371\n",
            "Recall is: 0.9322799097065463\n",
            "specificity is : 0.8419689119170984\n",
            "sensitivity is: 0.9322799097065463\n",
            "***********************************************************\n",
            "*********************************************\n",
            "time taken is: 131.24741720035672\n"
          ]
        }
      ],
      "source": [
        "# Use cross-validation on the training set to evaluate the performance of the logistic regression model for each value of C\n",
        "start=timer()\n",
        "soft_svm = LinearSVC(penalty='l2')\n",
        "grid_search = GridSearchCV(soft_svm, param_grid,  cv=5)\n",
        "grid_search.fit(X_train, Y_train)\n",
        "\n",
        "# Select the optimal value of C based on the performance of the model on the validation set\n",
        "best_C = grid_search.best_params_['C']\n",
        "print(f\"Best C: {best_C}\")\n",
        "\n",
        "# Train the final logistic regression model using the selected value of C and the entire training set\n",
        "final_soft_svm =  LinearSVC(penalty='l2',  C=best_C)\n",
        "final_soft_svm.fit(X_train, Y_train)\n",
        "\n",
        "# Evaluate the performance of the final model on the test set to estimate its performance on unseen data\n",
        "y_pred = final_soft_svm.predict(X_test)\n",
        "\n",
        "# for training data \n",
        "\n",
        "y_pred_train=final_soft_svm.predict(X_train)\n",
        "accuracy_soft_svm_l2_train = accuracy_score(Y_train, y_pred_train)\n",
        "precision_soft_svm_l2_train=precision_score(Y_train,y_pred_train)\n",
        "recall_soft_svm_l2_train=recall_score(Y_train,y_pred_train)\n",
        "# finding confusion matrix\n",
        "tn, fp, fn, tp = confusion_matrix(Y_train, y_pred_train).ravel()\n",
        "# specificity\n",
        "specificity_soft_svm_l2_train=tn/(tn+fp)\n",
        "# sensitivity\n",
        "sensitivity_soft_svm_l2_train=tp/(tp+fn)\n",
        "# print the all results\n",
        "print('For soft svm  using l2 penalty:\\n')\n",
        "print('for training data : \\n ************************************')\n",
        "print('Accuracy is:',accuracy_soft_svm_l2_train)\n",
        "print('Precision is:',precision_soft_svm_l2_train)\n",
        "print('Recall is:',recall_soft_svm_l2_train)\n",
        "print('specificity is :',specificity_soft_svm_l2_train)\n",
        "print('sensitivity is:',sensitivity_soft_svm_l2_train)\n",
        "\n",
        "# for test  data \n",
        "y_pred_test = final_soft_svm.predict(X_test)\n",
        "accuracy_soft_svm_l2_test = accuracy_score(Y_test, y_pred_test)\n",
        "precision_soft_svm_l2_test=precision_score(Y_test,y_pred_test)\n",
        "recall_soft_svm_l2_test=recall_score(Y_test,y_pred_test)\n",
        "# finding confusion matrix\n",
        "tn, fp, fn, tp = confusion_matrix(Y_test, y_pred_test).ravel()\n",
        "# specificity\n",
        "specificity_soft_svm_l2_test=tn/(tn+fp)\n",
        "# sensitivity\n",
        "sensitivity_soft_svm_l2_test=tp/(tp+fn)\n",
        "end=timer()\n",
        "# print the all results\n",
        "print('************************************************\\n for test data:')\n",
        "print('Accuracy is:',accuracy_soft_svm_l2_test)\n",
        "print('Precision is:',precision_soft_svm_l2_test)\n",
        "print('Recall is:',recall_soft_svm_l2_test)\n",
        "print('specificity is :',specificity_soft_svm_l2_test)\n",
        "print('sensitivity is:',sensitivity_soft_svm_l2_test)\n",
        "print('***********************************************************\\n*********************************************')\n",
        "print('time taken is:',end-start)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTBLEMfuYlBs"
      },
      "source": [
        "$\\huge \\text{Soft Margin SVM With L1 Norm:}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjqseYeoQiMp"
      },
      "source": [
        "$\\large  \\text{we set dual=False in this example because we are using L1 regularization with scikit-learn's LinearSVC function,} \\\\ \\large \\text{ and the dual algorithm is not supported with L1 regularization}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TgjC7kGqVaru"
      },
      "outputs": [],
      "source": [
        "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hX8JuMGNVaru"
      },
      "source": [
        "$\\large \\text{Reason for choosing these parameters:} \\\\ \\text{ Since we want to see the penalty effect over the prediction so we want to take a broad range of penalty so we started from } 10^{-3} \\text{and goes to} 10^4. \\\\  \\text{So that we can find the best parameter. }$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSknBGpgKn9Z",
        "outputId": "9e716412-a16d-43a5-b947-5dcafad403cb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/22n0451/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/home/22n0451/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/home/22n0451/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/home/22n0451/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/home/22n0451/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/home/22n0451/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/home/22n0451/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/home/22n0451/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/home/22n0451/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/home/22n0451/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/home/22n0451/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/home/22n0451/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/home/22n0451/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/home/22n0451/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/home/22n0451/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/home/22n0451/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/home/22n0451/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/home/22n0451/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/home/22n0451/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/home/22n0451/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/home/22n0451/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best C: 100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/22n0451/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "For soft svm  using l1 penalty:\n",
            "\n",
            "for training data : \n",
            " ************************************\n",
            "Accuracy is: 0.9993964996982498\n",
            "Precision is: 0.9994340690435767\n",
            "Recall is: 0.9994340690435767\n",
            "specificity is : 0.9993535875888817\n",
            "sensitivity is: 0.9994340690435767\n",
            "************************************************\n",
            " for test data:\n",
            "Accuracy is: 0.8890229191797346\n",
            "Precision is: 0.89086859688196\n",
            "Recall is: 0.9029345372460497\n",
            "specificity is : 0.8730569948186528\n",
            "sensitivity is: 0.9029345372460497\n",
            "***********************************************************\n",
            "*********************************************\n",
            "time taken is: 156.817613132298\n"
          ]
        }
      ],
      "source": [
        "# Use cross-validation on the training set to evaluate the performance of the logistic regression model for each value of C\n",
        "start=timer()\n",
        "soft_svm = LinearSVC(penalty='l1',dual=False )\n",
        "grid_search = GridSearchCV(soft_svm, param_grid,  cv=5)\n",
        "grid_search.fit(X_train, Y_train)\n",
        "\n",
        "# Select the optimal value of C based on the performance of the model on the validation set\n",
        "best_C = grid_search.best_params_['C']\n",
        "print(f\"Best C: {best_C}\")\n",
        "\n",
        "# Train the final logistic regression model using the selected value of C and the entire training set\n",
        "final_soft_svm = LinearSVC(penalty='l1',dual=False ,  C=best_C)\n",
        "final_soft_svm.fit(X_train, Y_train)\n",
        "\n",
        "# Evaluate the performance of the final model on the test set to estimate its performance on unseen data\n",
        "y_pred = final_soft_svm.predict(X_test)\n",
        "\n",
        "\n",
        "# for training data \n",
        "\n",
        "y_pred_train=final_soft_svm.predict(X_train)\n",
        "accuracy_soft_svm_l1_train = accuracy_score(Y_train, y_pred_train)\n",
        "precision_soft_svm_l1_train=precision_score(Y_train,y_pred_train)\n",
        "recall_soft_svm_l1_train=recall_score(Y_train,y_pred_train)\n",
        "# finding confusion matrix\n",
        "tn, fp, fn, tp = confusion_matrix(Y_train, y_pred_train).ravel()\n",
        "# specificity\n",
        "specificity_soft_svm_l1_train=tn/(tn+fp)\n",
        "# sensitivity\n",
        "sensitivity_soft_svm_l1_train=tp/(tp+fn)\n",
        "# print the all results\n",
        "print('For soft svm  using l1 penalty:\\n')\n",
        "print('for training data : \\n ************************************')\n",
        "print('Accuracy is:',accuracy_soft_svm_l1_train)\n",
        "print('Precision is:',precision_soft_svm_l1_train)\n",
        "print('Recall is:',recall_soft_svm_l1_train)\n",
        "print('specificity is :',specificity_soft_svm_l1_train)\n",
        "print('sensitivity is:',sensitivity_soft_svm_l1_train)\n",
        "\n",
        "# for test  data \n",
        "y_pred_test = final_soft_svm.predict(X_test)\n",
        "accuracy_soft_svm_l1_test = accuracy_score(Y_test, y_pred_test)\n",
        "precision_soft_svm_l1_test=precision_score(Y_test,y_pred_test)\n",
        "recall_soft_svm_l1_test=recall_score(Y_test,y_pred_test)\n",
        "# finding confusion matrix\n",
        "tn, fp, fn, tp = confusion_matrix(Y_test, y_pred_test).ravel()\n",
        "# specificity\n",
        "specificity_soft_svm_l1_test=tn/(tn+fp)\n",
        "# sensitivity\n",
        "sensitivity_soft_svm_l1_test=tp/(tp+fn)\n",
        "end=timer()\n",
        "# print the all results\n",
        "print('************************************************\\n for test data:')\n",
        "print('Accuracy is:',accuracy_soft_svm_l1_test)\n",
        "print('Precision is:',precision_soft_svm_l1_test)\n",
        "print('Recall is:',recall_soft_svm_l1_test)\n",
        "print('specificity is :',specificity_soft_svm_l1_test)\n",
        "print('sensitivity is:',sensitivity_soft_svm_l1_test)\n",
        "print('***********************************************************\\n*********************************************')\n",
        "print('time taken is:',end-start)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USq9FdtnYt6h"
      },
      "source": [
        "$\\huge \\text{Kernel SVM with Kernel RBF:}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "giZpcgT4Varv"
      },
      "outputs": [],
      "source": [
        "import multiprocessing as multiprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8kmwLfGVarv"
      },
      "source": [
        "$\\large \\text{Reason for choosing parameters:} \\\\ \\text{Our chose gamma parameters:param_grid =} [ 0.0001,0.001,0.01,0.1,1,10,100] \\\\ \\text{Reason: we know that  gamma paramter decides the decision boundary of classifier if it is small then decision boundary is smooth and if it is large} \\\\ \\text{ then decision boundary is complex which may lead to overfitting. So we here chosen more small values than large values }$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wY4zlvP_M-ZQ",
        "outputId": "dcf64355-9cf2-4138-b171-d0f7b3e94679"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best_gamma:0.01\n",
            "For kernel_SVM with rbf kernel penalty:\n",
            "\n",
            "for training data : \n",
            " ************************************\n",
            "Accuracy is: 0.9815932407966204\n",
            "Precision is: 0.9728381374722838\n",
            "Recall is: 0.9932088285229203\n",
            "specificity is : 0.9683257918552036\n",
            "sensitivity is: 0.9932088285229203\n",
            "************************************************\n",
            " for test data:\n",
            "Accuracy is: 0.8661037394451147\n",
            "Precision is: 0.8180076628352491\n",
            "Recall is: 0.963882618510158\n",
            "specificity is : 0.7538860103626943\n",
            "sensitivity is: 0.963882618510158\n",
            "***********************************************************\n",
            "*********************************************\n",
            "time taken is: 3158.902907986194\n"
          ]
        }
      ],
      "source": [
        "# Gamma values\n",
        "start=timer()\n",
        "param_grid = { 'gamma': [ 0.0001,0.001,0.01,0.1,1,10,100]}\n",
        "# Use cross-validation on the training set to evaluate the performance of the logistic regression model for each value of C\n",
        "kernel_SVM = SVC(kernel='rbf')\n",
        "grid_search = GridSearchCV(kernel_SVM, param_grid,  cv=5,n_jobs=multiprocessing.cpu_count())\n",
        "grid_search.fit(X_train, Y_train)\n",
        "\n",
        "# Select the optimal value of C based on the performance of the model on the validation set\n",
        "\n",
        "best_gamma=grid_search.best_params_['gamma']\n",
        "print(f\"Best_gamma:{best_gamma}\")\n",
        "\n",
        "# Train the final logistic regression model using the selected value of C and the entire training set\n",
        "final_kernel_SVM = SVC(kernel='rbf',gamma=best_gamma)\n",
        "final_kernel_SVM.fit(X_train, Y_train)\n",
        "\n",
        "# Evaluate the performance of the final model on the test set to estimate its performance on unseen data\n",
        "\n",
        "y_pred_train=final_kernel_SVM.predict(X_train)\n",
        "accuracy_kernel_SVM_train = accuracy_score(Y_train, y_pred_train)\n",
        "precision_kernel_SVM_train=precision_score(Y_train,y_pred_train)\n",
        "recall_kernel_SVM_train=recall_score(Y_train,y_pred_train)\n",
        "# finding confusion matrix\n",
        "tn, fp, fn, tp = confusion_matrix(Y_train, y_pred_train).ravel()\n",
        "# specificity\n",
        "specificity_kernel_SVM_train=tn/(tn+fp)\n",
        "# sensitivity\n",
        "sensitivity_kernel_SVM_train=tp/(tp+fn)\n",
        "# print the all results\n",
        "print('For kernel_SVM with rbf kernel penalty:\\n')\n",
        "print('for training data : \\n ************************************')\n",
        "print('Accuracy is:',accuracy_kernel_SVM_train)\n",
        "print('Precision is:',precision_kernel_SVM_train)\n",
        "print('Recall is:',recall_kernel_SVM_train)\n",
        "print('specificity is :',specificity_kernel_SVM_train)\n",
        "print('sensitivity is:',sensitivity_kernel_SVM_train)\n",
        "\n",
        "# for test  data \n",
        "y_pred_test = final_kernel_SVM.predict(X_test)\n",
        "accuracy_kernel_SVM_test = accuracy_score(Y_test, y_pred_test)\n",
        "precision_kernel_SVM_test=precision_score(Y_test,y_pred_test)\n",
        "recall_kernel_SVM_test=recall_score(Y_test,y_pred_test)\n",
        "# finding confusion matrix\n",
        "tn, fp, fn, tp = confusion_matrix(Y_test, y_pred_test).ravel()\n",
        "# specificity\n",
        "specificity_kernel_SVM_test=tn/(tn+fp)\n",
        "# sensitivity\n",
        "sensitivity_kernel_SVM_test=tp/(tp+fn)\n",
        "end=timer()\n",
        "# print the all results\n",
        "print('************************************************\\n for test data:')\n",
        "print('Accuracy is:',accuracy_kernel_SVM_test)\n",
        "print('Precision is:',precision_kernel_SVM_test)\n",
        "print('Recall is:',recall_kernel_SVM_test)\n",
        "print('specificity is :',specificity_kernel_SVM_test)\n",
        "print('sensitivity is:',sensitivity_kernel_SVM_test)\n",
        "print('***********************************************************\\n*********************************************')\n",
        "print('time taken is:',end-start)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VBEJJsmVarw"
      },
      "source": [
        "$\\huge \\text{K nearest Neighbours:} \\\\ \\text{Chosen parameters:}[1,2,3,4,5........,15] \\\\ \\text{Reason: Since we know that k nearest neighbours is a non parametrized method which is based on local density so as the value of k increases} \\\\ \\text{ it's accuracy sometimes may increase but sometime may decrease it depends over data set but here size of data set is not so large so range of 1} \\\\ \\text{ to 15 is good enough to predict.}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bAueE1ZfRu2d",
        "outputId": "8782145d-9eed-4bd2-fdde-3e8fe035da26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best K value: 1\n",
            "For k nearest neighbours penalty:\n",
            "\n",
            "for training data : \n",
            " ************************************\n",
            "Accuracy is: 0.9993964996982498\n",
            "Precision is: 0.9994340690435767\n",
            "Recall is: 0.9994340690435767\n",
            "specificity is : 0.9993535875888817\n",
            "sensitivity is: 0.9994340690435767\n",
            "************************************************\n",
            " for test data:\n",
            "Accuracy is: 0.7997587454764777\n",
            "Precision is: 0.9261538461538461\n",
            "Recall is: 0.6794582392776524\n",
            "specificity is : 0.9378238341968912\n",
            "sensitivity is: 0.6794582392776524\n",
            "***********************************************************\n",
            "*********************************************\n",
            "time taken is: 293.7335535660386\n"
          ]
        }
      ],
      "source": [
        "start=timer()\n",
        "# Define the KNN model\n",
        "knn = KNeighborsClassifier()\n",
        "\n",
        "# Define the parameter grid to search over\n",
        "param_grid = {'n_neighbors': range(1, 15)}\n",
        "\n",
        "# Perform a grid search over the parameter grid using 5-fold cross-validation\n",
        "grid_search = GridSearchCV(knn, param_grid, cv=5)\n",
        "\n",
        "# Fit the grid search to the data\n",
        "grid_search.fit(X_train, Y_train)\n",
        "\n",
        "# Print the best parameter setting\n",
        "print('best K value:',grid_search.best_params_['n_neighbors'])\n",
        "\n",
        "best_K=grid_search.best_params_['n_neighbors']\n",
        "# Train the final logistic regression model using the selected value of C and the entire training set\n",
        "final_knn = KNeighborsClassifier(best_K)\n",
        "final_knn.fit(X_train, Y_train)\n",
        "\n",
        "# Evaluate the performance of the final model on the test set to estimate its performance on unseen data\n",
        "# for training data \n",
        "\n",
        "y_pred_train=final_knn.predict(X_train)\n",
        "accuracy_knn_train = accuracy_score(Y_train, y_pred_train)\n",
        "precision_knn_train=precision_score(Y_train,y_pred_train)\n",
        "recall_knn_train=recall_score(Y_train,y_pred_train)\n",
        "# finding confusion matrix\n",
        "tn, fp, fn, tp = confusion_matrix(Y_train, y_pred_train).ravel()\n",
        "# specificity\n",
        "specificity_knn_train=tn/(tn+fp)\n",
        "# sensitivity\n",
        "sensitivity_knn_train=tp/(tp+fn)\n",
        "# print the all results\n",
        "print('For k nearest neighbours penalty:\\n')\n",
        "print('for training data : \\n ************************************')\n",
        "print('Accuracy is:',accuracy_knn_train)\n",
        "print('Precision is:',precision_knn_train)\n",
        "print('Recall is:',recall_knn_train)\n",
        "print('specificity is :',specificity_knn_train)\n",
        "print('sensitivity is:',sensitivity_knn_train)\n",
        "\n",
        "# for test  data \n",
        "y_pred_test = final_knn.predict(X_test)\n",
        "accuracy_knn_test = accuracy_score(Y_test, y_pred_test)\n",
        "precision_knn_test=precision_score(Y_test,y_pred_test)\n",
        "recall_knn_test=recall_score(Y_test,y_pred_test)\n",
        "# finding confusion matrix\n",
        "tn, fp, fn, tp = confusion_matrix(Y_test, y_pred_test).ravel()\n",
        "# specificity\n",
        "specificity_knn_test=tn/(tn+fp)\n",
        "# sensitivity\n",
        "sensitivity_knn_test=tp/(tp+fn)\n",
        "end=timer()\n",
        "# print the all results\n",
        "print('************************************************\\n for test data:')\n",
        "print('Accuracy is:',accuracy_knn_test)\n",
        "print('Precision is:',precision_knn_test)\n",
        "print('Recall is:',recall_knn_test)\n",
        "print('specificity is :',specificity_knn_test)\n",
        "print('sensitivity is:',sensitivity_knn_test)\n",
        "print('***********************************************************\\n*********************************************')\n",
        "print('time taken is:',end-start)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55Mhj4FKVarw"
      },
      "source": [
        "$\\huge \\text{Decision Tree Classifier:} \\\\ \\text{Chosen parameter (min weight fraction leaf)=}[0.0,0.01,0.1,0.25,0.5,0.75] \\\\ \\text{min weight fraction leaf value x shows each leaf node must have at least x percent of the total weighted data. So we take  initially 0 which mean} \\\\ \\text{ there is no limit over leaf for data but as we increase the parameter to see the the effect.}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_DzGHlwVarw",
        "outputId": "73e3b91f-28fd-42a3-dafd-93c88efc7967"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/22n0451/.local/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
            "5 fits failed out of a total of 30.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "5 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/22n0451/.local/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/home/22n0451/.local/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 889, in fit\n",
            "    super().fit(\n",
            "  File \"/home/22n0451/.local/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 177, in fit\n",
            "    self._validate_params()\n",
            "  File \"/home/22n0451/.local/lib/python3.9/site-packages/sklearn/base.py\", line 600, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/home/22n0451/.local/lib/python3.9/site-packages/sklearn/utils/_param_validation.py\", line 97, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'min_weight_fraction_leaf' parameter of DecisionTreeClassifier must be a float in the range [0.0, 0.5]. Got 0.75 instead.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/home/22n0451/.local/lib/python3.9/site-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.8654254  0.82619422 0.7163625  0.72239022 0.53319253        nan]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best min_weight_fraction_leaf: 0.0\n",
            "For Decision Tree Classifier:\n",
            "\n",
            "for training data : \n",
            " ************************************\n",
            "Accuracy is: 0.9993964996982498\n",
            "Precision is: 0.9994340690435767\n",
            "Recall is: 0.9994340690435767\n",
            "specificity is : 0.9993535875888817\n",
            "sensitivity is: 0.9994340690435767\n",
            "************************************************\n",
            " for test data:\n",
            "Accuracy is: 0.8600723763570567\n",
            "Precision is: 0.8531317494600432\n",
            "Recall is: 0.891647855530474\n",
            "specificity is : 0.8238341968911918\n",
            "sensitivity is: 0.891647855530474\n",
            "***********************************************************\n",
            "*********************************************\n",
            "time taken is: 33.02488311380148\n"
          ]
        }
      ],
      "source": [
        "\n",
        "start=timer()\n",
        "# Define the KNN model\n",
        "clf = DecisionTreeClassifier()\n",
        "# Define the parameter grid to search over\n",
        "param_grid = {'min_weight_fraction_leaf': [0.0,0.01,0.1,0.25,0.5,0.75]}\n",
        "\n",
        "# Perform a grid search over the parameter grid using 5-fold cross-validation\n",
        "grid_search = GridSearchCV(clf, param_grid, cv=5,n_jobs=multiprocessing.cpu_count())\n",
        "\n",
        "# Fit the grid search to the data\n",
        "grid_search.fit(X_train, Y_train)\n",
        "\n",
        "# Print the best parameter setting\n",
        "print('best min_weight_fraction_leaf:',grid_search.best_params_['min_weight_fraction_leaf'])\n",
        "\n",
        "best_min_weight=grid_search.best_params_['min_weight_fraction_leaf']\n",
        "# Train the final logistic regression model using the selected value of C and the entire training set\n",
        "final_clf = DecisionTreeClassifier(min_weight_fraction_leaf=best_min_weight)\n",
        "final_clf.fit(X_train, Y_train)\n",
        "\n",
        "# Evaluate the performance of the final model on the test set to estimate its performance on unseen data\n",
        "# for training data \n",
        "\n",
        "y_pred_train=final_clf.predict(X_train)\n",
        "accuracy_clf_train = accuracy_score(Y_train, y_pred_train)\n",
        "precision_clf_train=precision_score(Y_train,y_pred_train)\n",
        "recall_clf_train=recall_score(Y_train,y_pred_train)\n",
        "# finding confusion matrix\n",
        "tn, fp, fn, tp = confusion_matrix(Y_train, y_pred_train).ravel()\n",
        "# specificity\n",
        "specificity_clf_train=tn/(tn+fp)\n",
        "# sensitivity\n",
        "sensitivity_clf_train=tp/(tp+fn)\n",
        "# print the all results\n",
        "print('For Decision Tree Classifier:\\n')\n",
        "print('for training data : \\n ************************************')\n",
        "print('Accuracy is:',accuracy_clf_train)\n",
        "print('Precision is:',precision_clf_train)\n",
        "print('Recall is:',recall_clf_train)\n",
        "print('specificity is :',specificity_clf_train)\n",
        "print('sensitivity is:',sensitivity_clf_train)\n",
        "\n",
        "# for test  data \n",
        "y_pred_test = final_clf.predict(X_test)\n",
        "accuracy_clf_test = accuracy_score(Y_test, y_pred_test)\n",
        "precision_clf_test=precision_score(Y_test,y_pred_test)\n",
        "recall_clf_test=recall_score(Y_test,y_pred_test)\n",
        "# finding confusion matrix\n",
        "tn, fp, fn, tp = confusion_matrix(Y_test, y_pred_test).ravel()\n",
        "# specificity\n",
        "specificity_clf_test=tn/(tn+fp)\n",
        "# sensitivity\n",
        "sensitivity_clf_test=tp/(tp+fn)\n",
        "end=timer()\n",
        "# print the all results\n",
        "print('************************************************\\n for test data:')\n",
        "print('Accuracy is:',accuracy_clf_test)\n",
        "print('Precision is:',precision_clf_test)\n",
        "print('Recall is:',recall_clf_test)\n",
        "print('specificity is :',specificity_clf_test)\n",
        "print('sensitivity is:',sensitivity_clf_test)\n",
        "print('***********************************************************\\n*********************************************')\n",
        "print('time taken is:',end-start)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxok9CjUVarw"
      },
      "source": [
        "$\\huge  \\text{Random Forest Classifier} \\\\ \\text{As we want to choose the number of Trees whose low values may indicate underfitting and high values may indicate overfitting. }$ "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6QoR0EsVarx",
        "outputId": "2a1cdff5-6d10-444e-c0cc-f5524be1a9cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best n_estimators: 150\n",
            "For Random forest classifier:\n",
            "\n",
            "for training data : \n",
            " ************************************\n",
            "Accuracy is: 0.9993964996982498\n",
            "Precision is: 0.9994340690435767\n",
            "Recall is: 0.9994340690435767\n",
            "specificity is : 0.9993535875888817\n",
            "sensitivity is: 0.9994340690435767\n",
            "************************************************\n",
            " for test data:\n",
            "Accuracy is: 0.8950542822677925\n",
            "Precision is: 0.8617886178861789\n",
            "Recall is: 0.9571106094808126\n",
            "specificity is : 0.8238341968911918\n",
            "sensitivity is: 0.9571106094808126\n",
            "***********************************************************\n",
            "*********************************************\n",
            "time taken is: 109.03191603347659\n"
          ]
        }
      ],
      "source": [
        "\n",
        "start=timer()\n",
        "# Define the KNN model\n",
        "rd_forest=RandomForestClassifier()\n",
        "# Define the parameter grid to search over\n",
        "param_grid = {'n_estimators': [50,75,100,125,150,200,225]}\n",
        "\n",
        "# Perform a grid search over the parameter grid using 5-fold cross-validation\n",
        "grid_search = GridSearchCV(rd_forest, param_grid, cv=5,n_jobs=multiprocessing.cpu_count())\n",
        "\n",
        "# Fit the grid search to the data\n",
        "grid_search.fit(X_train, Y_train)\n",
        "\n",
        "# Print the best parameter setting\n",
        "print('best n_estimators:',grid_search.best_params_['n_estimators'])\n",
        "\n",
        "best_n_estimators=grid_search.best_params_['n_estimators']\n",
        "# Train the final logistic regression model using the selected value of C and the entire training set\n",
        "final_rd_forest =RandomForestClassifier(n_estimators=best_n_estimators)\n",
        "final_rd_forest.fit(X_train, Y_train)\n",
        "\n",
        "# Evaluate the performance of the final model on the test set to estimate its performance on unseen data\n",
        "# for training data \n",
        "\n",
        "y_pred_train=final_rd_forest.predict(X_train)\n",
        "accuracy_rd_forest_train = accuracy_score(Y_train, y_pred_train)\n",
        "precision_rd_forest_train=precision_score(Y_train,y_pred_train)\n",
        "recall_rd_forest_train=recall_score(Y_train,y_pred_train)\n",
        "# finding confusion matrix\n",
        "tn, fp, fn, tp = confusion_matrix(Y_train, y_pred_train).ravel()\n",
        "# specificity\n",
        "specificity_rd_forest_train=tn/(tn+fp)\n",
        "# sensitivity\n",
        "sensitivity_rd_forest_train=tp/(tp+fn)\n",
        "# print the all results\n",
        "print('For Random forest classifier:\\n')\n",
        "print('for training data : \\n ************************************')\n",
        "print('Accuracy is:',accuracy_rd_forest_train)\n",
        "print('Precision is:',precision_rd_forest_train)\n",
        "print('Recall is:',recall_rd_forest_train)\n",
        "print('specificity is :',specificity_rd_forest_train)\n",
        "print('sensitivity is:',sensitivity_rd_forest_train)\n",
        "\n",
        "# for test  data \n",
        "y_pred_test = final_rd_forest.predict(X_test)\n",
        "accuracy_rd_forest_test = accuracy_score(Y_test, y_pred_test)\n",
        "precision_rd_forest_test=precision_score(Y_test,y_pred_test)\n",
        "recall_rd_forest_test=recall_score(Y_test,y_pred_test)\n",
        "# finding confusion matrix\n",
        "tn, fp, fn, tp = confusion_matrix(Y_test, y_pred_test).ravel()\n",
        "# specificity\n",
        "specificity_rd_forest_test=tn/(tn+fp)\n",
        "# sensitivity\n",
        "sensitivity_rd_forest_test=tp/(tp+fn)\n",
        "end=timer()\n",
        "# print the all results\n",
        "print('************************************************\\n for test data:')\n",
        "print('Accuracy is:',accuracy_rd_forest_test)\n",
        "print('Precision is:',precision_rd_forest_test)\n",
        "print('Recall is:',recall_rd_forest_test)\n",
        "print('specificity is :',specificity_rd_forest_test)\n",
        "print('sensitivity is:',sensitivity_rd_forest_test)\n",
        "print('***********************************************************\\n*********************************************')\n",
        "print('time taken is:',end-start)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZeNMPBQVarx"
      },
      "source": [
        "$\\huge \\text{Que.E} \\\\  \\text{Tabulate the accuracy, precision, recall, specificity and senstivity values for training\n",
        "set T1 and test set T2 for each model trained in part (d) for} \\\\ \\text{ the best hyperparameter choices.\n",
        "Discuss your observations.}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvHPZoNdVarx"
      },
      "outputs": [],
      "source": [
        "headers=['Classifier name','Accuracy','Precision','Recall','Specificity','Sensitivity']\n",
        "data=[['Logistic Regression with penalty L2',accuracy_lr_l2_train,precision_lr_l2_train,recall_lr_l2_train,specificity_lr_l2_train,sensitivity_lr_l2_train],['Logistic Regression with penalty L1',accuracy_lr_l1_train,precision_lr_l1_train,recall_lr_l1_train,specificity_lr_l1_train,sensitivity_lr_l1_train],['Soft margin SVM with L2 norm',accuracy_soft_svm_l2_train,precision_soft_svm_l2_train,recall_soft_svm_l2_train,specificity_soft_svm_l2_train,sensitivity_soft_svm_l2_train],['Soft margin SVM with L1 norm',accuracy_soft_svm_l1_train,precision_soft_svm_l1_train,recall_soft_svm_l1_train,specificity_soft_svm_l1_train,sensitivity_soft_svm_l1_train],['Kernel SVM with Kernel RBF',accuracy_kernel_SVM_train,precision_kernel_SVM_train,recall_kernel_SVM_train,specificity_kernel_SVM_train,sensitivity_kernel_SVM_train],['K nearest Neighbours',accuracy_knn_train,precision_knn_train,recall_knn_train,specificity_knn_train,sensitivity_knn_train],['Decision Tree Classifier',accuracy_clf_train,precision_clf_train,recall_clf_train,specificity_clf_train,sensitivity_clf_train],['Random forest classifier',accuracy_rd_forest_train,precision_rd_forest_train,recall_rd_forest_train,specificity_rd_forest_train,sensitivity_rd_forest_train]]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_GrqQaDVarx"
      },
      "outputs": [],
      "source": [
        "table_train=pd.DataFrame(data,columns=headers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhFEVMpJVarx",
        "outputId": "7237152b-56a0-4087-ce82-e5aebd7d645c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Classifier name</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Specificity</th>\n",
              "      <th>Sensitivity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Logistic Regression with penalty L2</td>\n",
              "      <td>0.998189</td>\n",
              "      <td>0.998302</td>\n",
              "      <td>0.998302</td>\n",
              "      <td>0.998061</td>\n",
              "      <td>0.998302</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Logistic Regression with penalty L1</td>\n",
              "      <td>0.999396</td>\n",
              "      <td>0.999434</td>\n",
              "      <td>0.999434</td>\n",
              "      <td>0.999354</td>\n",
              "      <td>0.999434</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Soft margin SVM with L2 norm</td>\n",
              "      <td>0.990344</td>\n",
              "      <td>0.988732</td>\n",
              "      <td>0.993209</td>\n",
              "      <td>0.987072</td>\n",
              "      <td>0.993209</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Soft margin SVM with L1 norm</td>\n",
              "      <td>0.999396</td>\n",
              "      <td>0.999434</td>\n",
              "      <td>0.999434</td>\n",
              "      <td>0.999354</td>\n",
              "      <td>0.999434</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Kernel SVM with Kernel RBF</td>\n",
              "      <td>0.981593</td>\n",
              "      <td>0.972838</td>\n",
              "      <td>0.993209</td>\n",
              "      <td>0.968326</td>\n",
              "      <td>0.993209</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>K nearest Neighbours</td>\n",
              "      <td>0.999396</td>\n",
              "      <td>0.999434</td>\n",
              "      <td>0.999434</td>\n",
              "      <td>0.999354</td>\n",
              "      <td>0.999434</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Decision Tree Classifier</td>\n",
              "      <td>0.999396</td>\n",
              "      <td>0.999434</td>\n",
              "      <td>0.999434</td>\n",
              "      <td>0.999354</td>\n",
              "      <td>0.999434</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Random forest classifier</td>\n",
              "      <td>0.999396</td>\n",
              "      <td>0.999434</td>\n",
              "      <td>0.999434</td>\n",
              "      <td>0.999354</td>\n",
              "      <td>0.999434</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       Classifier name  Accuracy  Precision    Recall  \\\n",
              "0  Logistic Regression with penalty L2  0.998189   0.998302  0.998302   \n",
              "1  Logistic Regression with penalty L1  0.999396   0.999434  0.999434   \n",
              "2         Soft margin SVM with L2 norm  0.990344   0.988732  0.993209   \n",
              "3         Soft margin SVM with L1 norm  0.999396   0.999434  0.999434   \n",
              "4           Kernel SVM with Kernel RBF  0.981593   0.972838  0.993209   \n",
              "5                 K nearest Neighbours  0.999396   0.999434  0.999434   \n",
              "6             Decision Tree Classifier  0.999396   0.999434  0.999434   \n",
              "7             Random forest classifier  0.999396   0.999434  0.999434   \n",
              "\n",
              "   Specificity  Sensitivity  \n",
              "0     0.998061     0.998302  \n",
              "1     0.999354     0.999434  \n",
              "2     0.987072     0.993209  \n",
              "3     0.999354     0.999434  \n",
              "4     0.968326     0.993209  \n",
              "5     0.999354     0.999434  \n",
              "6     0.999354     0.999434  \n",
              "7     0.999354     0.999434  "
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "table_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JpNyUK_qVarx"
      },
      "outputs": [],
      "source": [
        "headers=['Classifier name','Accuracy','Precision','Recall','Specificity','Sensitivity']\n",
        "data_test=[['Logistic Regression with penalty L2',accuracy_lr_l2_test,precision_lr_l2_test,recall_lr_l2_test,specificity_lr_l2_test,sensitivity_lr_l2_test],['Logistic Regression with penalty L1',accuracy_lr_l1_test,precision_lr_l1_test,recall_lr_l1_test,specificity_lr_l1_test,sensitivity_lr_l1_test],['Soft margin SVM with L2 norm',accuracy_soft_svm_l2_test,precision_soft_svm_l2_test,recall_soft_svm_l2_test,specificity_soft_svm_l2_test,sensitivity_soft_svm_l2_test],['Soft margin SVM with L1 norm',accuracy_soft_svm_l1_test,precision_soft_svm_l1_test,recall_soft_svm_l1_test,specificity_soft_svm_l1_test,sensitivity_soft_svm_l1_test],['Kernel SVM with Kernel RBF',accuracy_kernel_SVM_test,precision_kernel_SVM_test,recall_kernel_SVM_test,specificity_kernel_SVM_test,sensitivity_kernel_SVM_test],['K nearest Neighbours',accuracy_knn_test,precision_knn_test,recall_knn_test,specificity_knn_test,sensitivity_knn_test],['Decision Tree Classifier',accuracy_clf_test,precision_clf_test,recall_clf_test,specificity_clf_test,sensitivity_clf_test],['Random forest classifier',accuracy_rd_forest_test,precision_rd_forest_test,recall_rd_forest_test,specificity_rd_forest_test,sensitivity_rd_forest_test]]\n",
        "table_test=pd.DataFrame(data_test,columns=headers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WrsHv6jeVarx",
        "outputId": "4789a032-329b-43df-860c-18a737dc0bb0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Classifier name</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Specificity</th>\n",
              "      <th>Sensitivity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Logistic Regression with penalty L2</td>\n",
              "      <td>0.891435</td>\n",
              "      <td>0.881210</td>\n",
              "      <td>0.920993</td>\n",
              "      <td>0.857513</td>\n",
              "      <td>0.920993</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Logistic Regression with penalty L1</td>\n",
              "      <td>0.884198</td>\n",
              "      <td>0.886414</td>\n",
              "      <td>0.898420</td>\n",
              "      <td>0.867876</td>\n",
              "      <td>0.898420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Soft margin SVM with L2 norm</td>\n",
              "      <td>0.890229</td>\n",
              "      <td>0.871308</td>\n",
              "      <td>0.932280</td>\n",
              "      <td>0.841969</td>\n",
              "      <td>0.932280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Soft margin SVM with L1 norm</td>\n",
              "      <td>0.889023</td>\n",
              "      <td>0.890869</td>\n",
              "      <td>0.902935</td>\n",
              "      <td>0.873057</td>\n",
              "      <td>0.902935</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Kernel SVM with Kernel RBF</td>\n",
              "      <td>0.866104</td>\n",
              "      <td>0.818008</td>\n",
              "      <td>0.963883</td>\n",
              "      <td>0.753886</td>\n",
              "      <td>0.963883</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>K nearest Neighbours</td>\n",
              "      <td>0.799759</td>\n",
              "      <td>0.926154</td>\n",
              "      <td>0.679458</td>\n",
              "      <td>0.937824</td>\n",
              "      <td>0.679458</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Decision Tree Classifier</td>\n",
              "      <td>0.860072</td>\n",
              "      <td>0.853132</td>\n",
              "      <td>0.891648</td>\n",
              "      <td>0.823834</td>\n",
              "      <td>0.891648</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Random forest classifier</td>\n",
              "      <td>0.895054</td>\n",
              "      <td>0.861789</td>\n",
              "      <td>0.957111</td>\n",
              "      <td>0.823834</td>\n",
              "      <td>0.957111</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       Classifier name  Accuracy  Precision    Recall  \\\n",
              "0  Logistic Regression with penalty L2  0.891435   0.881210  0.920993   \n",
              "1  Logistic Regression with penalty L1  0.884198   0.886414  0.898420   \n",
              "2         Soft margin SVM with L2 norm  0.890229   0.871308  0.932280   \n",
              "3         Soft margin SVM with L1 norm  0.889023   0.890869  0.902935   \n",
              "4           Kernel SVM with Kernel RBF  0.866104   0.818008  0.963883   \n",
              "5                 K nearest Neighbours  0.799759   0.926154  0.679458   \n",
              "6             Decision Tree Classifier  0.860072   0.853132  0.891648   \n",
              "7             Random forest classifier  0.895054   0.861789  0.957111   \n",
              "\n",
              "   Specificity  Sensitivity  \n",
              "0     0.857513     0.920993  \n",
              "1     0.867876     0.898420  \n",
              "2     0.841969     0.932280  \n",
              "3     0.873057     0.902935  \n",
              "4     0.753886     0.963883  \n",
              "5     0.937824     0.679458  \n",
              "6     0.823834     0.891648  \n",
              "7     0.823834     0.957111  "
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "table_test"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\huge \\text{Results for training data for various methods:}$"
      ],
      "metadata": {
        "id": "bhP_s45qbv-x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "|index|Classifier name|Accuracy|Precision|Recall|Specificity|Sensitivity|\n",
        "|---|---|---|---|---|---|---|\n",
        "|0|Logistic Regression with penalty L2|0\\.998189|0\\.998302|0\\.998302|0\\.998061|0\\.998302|\n",
        "|1|Logistic Regression with penalty L1|0\\.999396|0\\.999434|0\\.999434|0\\.999354|0\\.999434|\n",
        "|2|Soft margin SVM with L2 norm|0\\.990344|0\\.988732|0\\.993209|0\\.987072|0\\.993209|\n",
        "|3|Soft margin SVM with L1 norm|0\\.999396|0\\.999434|0\\.999434|0\\.999354|0\\.999434|\n",
        "|4|Kernel SVM with Kernel RBF|0\\.981593|0\\.972838|0\\.993209|0\\.968326|0\\.993209|\n",
        "|5|K nearest Neighbours|0\\.999396|0\\.999434|0\\.999434|0\\.999354|0\\.999434|\n",
        "|6|Decision Tree Classifier|0\\.999396|0\\.999434|0\\.999434|0\\.999354|0\\.999434|\n",
        "|7|Random forest classifier|0\\.999396|0\\.999434|0\\.999434|0\\.999354|0\\.999434|"
      ],
      "metadata": {
        "id": "GwGad01nbivS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\large \\text{Observations:} \\\\ \\text{Here we can see that All the scores (accuracy, Precision, Recall, Specificity, Sensitivity, Sensitivity) are about to it's ideal values which } \\\\ \\text{indicates that by cross validation we got an appropriate parameter which gives suitable results.}$"
      ],
      "metadata": {
        "id": "jwX04kR2b_4g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\huge \\text{Results for test data for various methods:}$"
      ],
      "metadata": {
        "id": "oY-xypyXb3J3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "|index|Classifier name|Accuracy|Precision|Recall|Specificity|Sensitivity|\n",
        "|---|---|---|---|---|---|---|\n",
        "|0|Logistic Regression with penalty L2|0\\.891435|0\\.88121|0\\.920993|0\\.857513|0\\.920993|\n",
        "|1|Logistic Regression with penalty L1|0\\.884198|0\\.886414|0\\.89842|0\\.867876|0\\.89842|\n",
        "|2|Soft margin SVM with L2 norm|0\\.890229|0\\.871308|0\\.93228|0\\.841969|0\\.93228|\n",
        "|3|Soft margin SVM with L1 norm|0\\.889023|0\\.890869|0\\.902935|0\\.873057|0\\.902935|\n",
        "|4|Kernel SVM with Kernel RBF|0\\.866104|0\\.818008|0\\.963883|0\\.753886|0\\.963883|\n",
        "|5|K nearest Neighbours|0\\.799759|0\\.926154|0\\.679458|0\\.937824|0\\.679458|\n",
        "|6|Decision Tree Classifier|0\\.860072|0\\.853132|0\\.891648|0\\.823834|0\\.891648|\n",
        "|7|Random forest classifier|0\\.895054|0\\.861789|0\\.957111|0\\.823834|0\\.957111|"
      ],
      "metadata": {
        "id": "_q_gCNmWdijb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\large \\text{Observation:} \\\\ \\text{As we can see that Various methods are giving various reasons.} \\\\ \\text{The best performing models in terms of sensitivity are the Soft margin SVM with L2 norm and the Random forest classifier, with accuracy scores} \\\\ \\text{ of 0.93228 and 0.957111 respectively.} \\\\ \\text{The K nearest Neighbours model has the highest score on the recall metric, which measures the ability of the model to identify all relevant instances.} \\\\ \\text{The Kernel SVM with Kernel RBF has the lowest accuracy score of 0.866104, indicating that this model may not be the best fit for the dataset.}$"
      ],
      "metadata": {
        "id": "w7kNXn-dc_AG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\huge \\text{Q.F} \\\\ \\text{Discuss if the L1 regularizers used in logistic regression and soft-margin SVM resulted\n",
        "in sparse models when compared to L2 regularizers.} \\\\ \\text{ Also compare and contrast the performance\n",
        "of the models obtained using L1 and L2 regularizers. Using these observations, what\n",
        "would you} \\\\ \\text{ suggest to the practitioner regarding the use of L1 regularizer?}$"
      ],
      "metadata": {
        "id": "wXnPvclMfldg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\large \\text{For Logistic Regression:} \\\\ \\text{L1 norm: As we can see that every score what we computed (accuracy, Precision, Recall, Specificity, Sensitivity, Sensitivity) is lower than} \\\\ \\text{ scores obtained while using L2 norm in unseen data(test data). Which indicates that for our data set Logistic regression with L2 norm is} \\\\ \\text{ working better than l1 norm. } \\\\ \\large \\text{For SVM :} \\\\ \\text{As we can see that various types of results are behaving differently  like l1 norm is giving precision and specificity better than l2 penalty that} \\\\ \\text{ is if we are interested in only correctly predicted data in all data then l1 norm  is better than l2 norm.} \\\\ \\text{On the other hand SVM with l2 penalty is giving better recall and sensitivity than l1 penalty that means if we are interested in } \\\\ \\text{the correctly predicted data points in all predicted data points in that class than l2 penalty is better than l1 norm. } \\\\ \\text{As we can see that choosing l1 or l2 depends on the observers choice.}$"
      ],
      "metadata": {
        "id": "f6b0DjNCgSKX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\large \\text{suggestion to the practitioner regarding the use of L1 regularizer :} \\\\ \\text{For Logistic regression: In logistic regression using l1 at the place of l2 is not a good choice since we saw the all scores obtained by l1 } \\\\ \\text{penalized score is less than l2 penalized scores.}$"
      ],
      "metadata": {
        "id": "Sh0QLp_-hjLV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2SOcK16uddsr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}